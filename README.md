# Ecommerce Pipeline Project

## Table of Contents

- Kinesis Pipeline[#kinesis-pipeline]
- Ingestion[#ingestion]
- Lake(#lake)

- Virtual Environment and Package Management[#virtual-environment-and-package-management]

## Kinesis Pipeline

- Architecture:

  1. kinesis data stream
  2. kinesis firehose (a placeholder lambda function for stream processing -- enrich or filter data)
  3. s3 bucket for raw data

## Ingestion

- producer: a python script used to send 20 testing records from the dataset to our `Kinesis_Pipeline`.

  > Note: before running the command: `python ingestion/streaming_ingestion/producer.py`, please go download the dataset: **Online Retail.xlsx** from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail), and then put the file under the `/data` folder.

- batch_loader: a python script used to the full dataset directly to the lake(s3 bucket) as one file or split files -- simulating a bulk upload process

## Lake

- `lake/glue_catalog/`: CDK stack for Glue Catalog and Crawlers
  - a glue catalog database: a place to store all _Metadata Tables_ generated by Glue Crawlers
  - a glue crawler: an automated tools used to access your data store, identifies metadata, and creates table definitions in the AWS Glue Data Catalog
    > note: The AWS Glue Data Catalog is an index of your data’s location, schema, and runtime metrics.
- `lake/glue_jobs/`: PySpark or Python-based Glue ETL jobs

  - in our case, we have a python shell script as our etl job script for local testing purpose. We could convert it to AWS Glue ETL Job if need or switch to PySpark Job. For this project, we stick with the python shell script to keep it simple and cost-efficient.
    > note: PySpark is working on a distributed computing system, which is ideal for heavy data processing jobs.

- **BeAware**: in the Layer:Lake, we use a python shell etl job to scan the raw data stored in the s3 bucket. While in the real-world production systems, we often clean or transform data before making it queryable. However, still there are some reasons we would crawl raw data:
  - `Exploratory access`: Crawling raw data lets analysts/data engineers inspect the data early via Athena, even before ETL jobs exist.
  - `Schema inference`: Crawlers give you automatic schema discovery. This is super helpful for messy CSVs, external vendors, or IoT logs.
  - `Backfill scenarios`: If you receive historical data (e.g., monthly CSV dumps), you can crawl and immediately make them queryable before cleaning.
  - `Glue Catalog integration`: Even raw data tables are needed if you’re building full lineage tracking or a central catalog.
  - `Learning/debugging`: Crawling raw formats helps beginners understand partitioning, formats, compression effects, etc.

<!-- - `ml/inference/`: Batch/real-time inference code using trained models
- `rag/app/`: Frontend + API layer for demoing RAG interactions -->

## Virtual Environment and Package Management

- Current Design: For package management, this project is using one single virtual environment, but with multiple requirements.txt files. As such, we could relatively easily manage packages for each of our sub projects as well as the main project. In order to do it, we use a tool called `pip-compile` with multiple `requirements.in` files.

- Future Challenge & Solution: Right now, we are using one single virtual environment for our project, which consists of mutliple sub projects. However, as our project grows, we may encounter issues such as `Potential version conflicts`. And when that moment comes, we could switch to a `Layered` approach with `pip install -e`. In other words, we can make the main project installable as a package, then install it in subprojects using editable mode. By doing this, our code base will be more `Clean, modular, real-Pythonic`.
  > Note: since we are already using `pip-compile`, it will make later migration more smooth because every sub project has its own `requirements.in`. So with this setting, it is safe, compatible, future-proof.
