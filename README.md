# Ecommerce Pipeline Project

## Table of Contents

- [Kinesis Pipeline](#kinesis-pipeline)
- [Ingestion](#ingestion)
- [Lake](#lake)
- [Analytic](#analytics)

- [Virtual Environment and Package Management](#virtual-environment-and-package-management)

## Kinesis Pipeline

- Architecture:

> Note: we are now sticking with manual ETL job(`lake/glue_jobs/etl_raw_to_clean.py`). While for automating ETL processing, or setting S3 notification with ETL processing lambda, we must tweak the settings of the lambda (`memory`, `timeout`, `logger`) first, so that the reading and writing results can be seen more easily.

1. kinesis data stream
2. kinesis firehose (a placeholder lambda function for stream processing -- enrich or filter data)
3. s3 bucket for raw data
4. s3-notification lambda for automating ETL processing (commented out from the stack)
   > Note: when dealing with S3-Notification, be aware that the **S3 object key** (i.e. the file path) from the event record is **URL-encoded**( characters like `space` will be encoded to `%20`), DO NOT forget to decode it before using it to fetch objects from the bucket.
   > Note: for this lambda, we have to use packages: `awswrangler`,`pandas`, which are quite large for bundling, so we choose to use AWS pre-built layer.

- **BeAware**:
  - Note: for python lambda function, there is NO native `esbuild` equivalent like `NodejsFunction`, we have to either bundle the function and its dependencies manually or bundle them via `Docker`.
  - Note: for python lambda function, if choosing to bundle function and dependencies manually, it is possible to encounter a type of issues, which may look like this `Runtime.ImportModuleError: Unable to import module 'etl_handler': Error importing numpy: you should not try to import numpy from...`.
    - Reason: the packages like `numpy` will be installed with a specific version according to the underlying machines(Windows, MacOS, Linux), AWS Lambda is using Linux and if we use different OS on our local machine, then it will fail.
    - Solution: use the command: `pip install -r requirements.txt --platform manylinux2014_x86_64 --only-binary=:all: -t python/`. Basically what you do is force a linux distribution.
      - [related github repo issue](https://github.com/numpy/numpy/issues/13465)
      - [AWS Lambda -- Python Layer](https://docs.aws.amazon.com/lambda/latest/dg/python-layers.html)
  - Note: for python function or other types of function, we could install some very large packages, such `pyarrow`, `pandas`, etc. And we have a limit of 250mb lambda size, and in order to go over this limit, we have to use `Layers`.
    - There are some AWS pre-built layers that we can use. The following link shows how to add layers from the lambda console as well as how to check the `ARN`s of those layers: [Lambda Layers](https://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components/)

## Ingestion

- producer: a python script used to send 20 testing records from the dataset to our `Kinesis_Pipeline`.

  > Note: before running the command: `python ingestion/streaming_ingestion/producer.py`, please go download the dataset: **Online Retail.xlsx** from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail), and then put the file under the `/data` folder.

- batch_loader: a python script used to the full dataset directly to the lake(s3 bucket) as one file or split files -- simulating a bulk upload process

## Lake

- `lake/glue_catalog/`: CDK stack for Glue Catalog and Crawlers
  - a glue catalog database: a place to store all _Metadata Tables_ generated by Glue Crawlers
  - a glue crawler: an automated tools used to access your data store, identifies metadata, and creates table definitions in the AWS Glue Data Catalog
    > note: The AWS Glue Data Catalog is an index of your data’s location, schema, and runtime metrics.
- `lake/glue_jobs/`: PySpark or Python-based Glue ETL jobs

  - in our case, we have a python shell script as our etl job script for local testing purpose. We could convert it to AWS Glue ETL Job if need or switch to PySpark Job. For this project, we stick with the python shell script to keep it simple and cost-efficient.
    > note: PySpark is working on a distributed computing system, which is ideal for heavy data processing jobs.

- **BeAware**: in the Layer:Lake, we use a python shell etl job to scan the raw data stored in the s3 bucket. While in the real-world production systems, we often clean or transform data before making it queryable. However, still there are some reasons we would crawl raw data:
  - `Exploratory access`: Crawling raw data lets analysts/data engineers inspect the data early via Athena, even before ETL jobs exist.
  - `Schema inference`: Crawlers give you automatic schema discovery. This is super helpful for messy CSVs, external vendors, or IoT logs.
  - `Backfill scenarios`: If you receive historical data (e.g., monthly CSV dumps), you can crawl and immediately make them queryable before cleaning.
  - `Glue Catalog integration`: Even raw data tables are needed if you’re building full lineage tracking or a central catalog.
  - `Learning/debugging`: Crawling raw formats helps beginners understand partitioning, formats, compression effects, etc.

## Analytics

- Athena Queries:
  - daily sales
  - popular products
  - sales by country
  - top customers
- Quicksight Charts:
  <p align="center">
    <img src="/analytics/quicksight_imgs/daily_sales.png" alt="daily_sales" width="700" height="430">
  </p> 
  <p align="center">
    <img src="/analytics//quicksight_imgs/popular_products.png" alt="popular_products" width="700" height="430">
  </p> 
  <p align="center">
    <img src="/analytics//quicksight_imgs/sales_by_country.png" alt="sales_by_country" width="700" height="430">
  </p> 
  <p align="center">
    <img src="/analytics//quicksight_imgs/top_customers.png" alt="top_customers" width="700" height="430">
  </p>

<!-- - `ml/inference/`: Batch/real-time inference code using trained models
- `rag/app/`: Frontend + API layer for demoing RAG interactions -->

## Virtual Environment and Package Management

- Current Design: For package management, this project is using one single virtual environment, but with multiple requirements.txt files. As such, we could relatively easily manage packages for each of our sub projects as well as the main project. In order to do it, we use a tool called `pip-compile` with multiple `requirements.in` files.

- Future Challenge & Solution: Right now, we are using one single virtual environment for our project, which consists of mutliple sub projects. However, as our project grows, we may encounter issues such as `Potential version conflicts`. And when that moment comes, we could switch to a `Layered` approach with `pip install -e`. In other words, we can make the main project installable as a package, then install it in subprojects using editable mode. By doing this, our code base will be more `Clean, modular, real-Pythonic`.
  > Note: since we are already using `pip-compile`, it will make later migration more smooth because every sub project has its own `requirements.in`. So with this setting, it is safe, compatible, future-proof.
