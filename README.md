# Ecommerce Pipeline Project

## Introduction

This project is an end-to-end ecommerce data pipeline and AI system designed to simulate real-world scenarios involving data engineering and AI/ML/RAG applications. It serves as a hands-on learning and experimentation ground, especially for beginners and practitioners who want to understand the full pipeline from raw data ingestion to AI-driven product recommendations and question answering.

The system integrates multiple layers:

- Ingestion Layer: Simulates streaming ingestion with Kinesis.
- Lake Layer: Manages raw and structured data with S3, Glue, and Athena.
- Analytics Layer: Provides SQL-based dashboards and business insights via AWS Quicksight.
- ML Layer: Builds models for classification, regression, and recommendations.
- Notebook Layer: Supports EDA, profiling, cleaning, feature engineering, and visualization.
- RAG Layer: Enables product Q&A using LLMs and vector similarity search with FAISS.

Each layer is modular but interconnected, and most rely on artifacts produced by earlier stages.

## Table of Contents

- [How to use](#how-to-use)
- [Kinesis Pipeline](#kinesis-pipeline)
- [Ingestion](#ingestion)
- [Lake](#lake)
- [Analytic](#analytics)
- [ML](#ml)
- [Notebooks](#notebooks)
- [RAG](#rag)

- [Virtual Environment and Package Management](#virtual-environment-and-package-management)

## How to use

1. set up the main environment for the project

- Create a Python virtual environment at the root level:
  ```bash
  python3.12 -m venv .venv
  source .venv/bin/activate
  pip install -r requirements.txt
  ```
- Use separated vitual environments for special cases:
  - `notebooks/` uses its own venv due to ydata-profiling.
  - `rag/app` uses its own venv due to vector DB and LLM setup.
- Each of these folders includes its own requirements.in and requirements.txt.

2. workflow by layers

- Ingestion Layer:
  - use AWS CDK to provision necessary resources(`infrastructure/kinesis_pipeline`):
    - a `kinesis data stream`
    - a `kinesis firehose configuration`
    - a `s3 target bucket` (store raw data as well as clean data)
  - simulate data transferring(`ingestion/`)
    - via kinesis stream: `streaming_ingestion/`
    - directly to data lake(s3 bucket): `batch_ingestion/`
- Lake Layer(`lake/`):
  - Raw data lands in S3.
  - Glue Crawlers and Glue Jobs catalog and transform the data.
  - Resulting tables are queried using Athena.
- Analytics Layer (`analytics/`)
  - Athena queries produce business metrics like top countries, products, revenue, etc.
  - Dashboards can be created in QuickSight.
- ML Layer(`ml/`):
  - Includes training scripts for:
    - Binary Classification
    - Sales Prediction
    - Product Recommendation (ALS model)
  - Artifacts like `joblib` files and `latent vectors` are saved under `ml/recommendation/model/`.
- Notebook Layer(`notebooks/`)
  - Perform EDA, profiling, data cleaning, feature engineering, and visualization.
- RAG Layer(`rag/app/`)
  - Converts `temp.csv`(raw data csv file, generated by the `ingestion/batch_ingestion/batch_loader.py`) into a Markdown product knowledge file.
  - Splits and embeds content using `HuggingFace Embeddings`.
  - Stores vector index using `FAISS`(used to similarity search and vector storage).
  - Uses `OpenAI GPT` to answer questions over the indexed content.(needs `OpenAI API Key`)

3. Notes

- Set up your `.env` files with secrets like `OPENAI_API_KEY`.
- Always remember to install packages for each sub project via command: `pip install -r <path>/requirements.txt` before running scripts or testing projects.
- Adjust paths accordingly if you're running from different working directories.
- Always activate the correct virtual environment before running any scripts.

## Kinesis Pipeline

- Architecture:

> Note: we are now sticking with manual ETL job(`lake/glue_jobs/etl_raw_to_clean.py`), because for automating ETL processing, or setting S3 notification with ETL processing lambda, we must tweak the settings of the lambda (`memory`, `timeout`, `logger`) first, so that the reading and writing results can be seen more easily.

1. kinesis data stream
2. kinesis firehose (a placeholder lambda function for stream processing -- enrich or filter data)
3. s3 bucket for raw data
4. s3-notification lambda for automating ETL processing (commented out from the stack)
   > Note: when dealing with S3-Notification, be aware that the **S3 object key** (i.e. the file path) from the event record is **URL-encoded**( characters like `space` will be encoded to `%20`), DO NOT forget to decode it before using it to fetch objects from the bucket.
   > Note: for this lambda, we have to use packages: `awswrangler`,`pandas`, which are quite large for bundling, so we choose to use AWS pre-built layer.

- **BeAware**:
  - Note: for python lambda function, there is NO native `esbuild` equivalent like `NodejsFunction`, we have to either bundle the function and its dependencies manually or bundle them via `Docker`.
  - Note: for python lambda function, if choosing to bundle function and dependencies manually, it is possible to encounter a type of issues, which may look like this `Runtime.ImportModuleError: Unable to import module 'etl_handler': Error importing numpy: you should not try to import numpy from...`.
    - Reason: the packages like `numpy` will be installed with a specific version according to the underlying machines(Windows, MacOS, Linux), AWS Lambda is using Linux and if we use different OS on our local machine, then it will fail.
    - Solution: use the command: `pip install -r requirements.txt --platform manylinux2014_x86_64 --only-binary=:all: -t python/`. Basically what you do is force a linux distribution.
      - [related github repo issue](https://github.com/numpy/numpy/issues/13465)
      - [AWS Lambda -- Python Layer](https://docs.aws.amazon.com/lambda/latest/dg/python-layers.html)
  - Note: for python function or other types of function, we could install some very large packages, such `pyarrow`, `pandas`, etc. And we have a limit of 250mb lambda size, and in order to go over this limit, we have to use `Layers`.
    - There are some AWS pre-built layers that we can use. The following link shows how to add layers from the lambda console as well as how to check the `ARN`s of those layers: [Lambda Layers](https://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components/)

## Ingestion

- producer: a python script used to send 20 testing records from the dataset to our `Kinesis_Pipeline`.

  > Note: before running the command: `python ingestion/streaming_ingestion/producer.py`, please go download the dataset: **Online Retail.xlsx** from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail), and then put the file under the `/data` folder.

- batch_loader: a python script used to the full dataset directly to the lake(s3 bucket) as one file or split files -- simulating a bulk upload process

## Lake

- `lake/glue_catalog/`: CDK stack for Glue Catalog and Crawlers
  - a glue catalog database: a place to store all _Metadata Tables_ generated by Glue Crawlers
  - a glue crawler: an automated tools used to access your data store, identifies metadata, and creates table definitions in the AWS Glue Data Catalog
    > note: The AWS Glue Data Catalog is an index of your data’s location, schema, and runtime metrics.
- `lake/glue_jobs/`: PySpark or Python-based Glue ETL jobs

  - in our case, we have a python shell script as our etl job script for local testing purpose. We could convert it to AWS Glue ETL Job if need or switch to PySpark Job. For this project, we stick with the python shell script to keep it simple and cost-efficient.
    > note: PySpark is working on a distributed computing system, which is ideal for heavy data processing jobs.

- **BeAware**: in the Layer:Lake, we use a python shell etl job to scan the raw data stored in the s3 bucket. While in the real-world production systems, we often clean or transform data before making it queryable. However, still there are some reasons we would crawl raw data:
  - `Exploratory access`: Crawling raw data lets analysts/data engineers inspect the data early via Athena, even before ETL jobs exist.
  - `Schema inference`: Crawlers give you automatic schema discovery. This is super helpful for messy CSVs, external vendors, or IoT logs.
  - `Backfill scenarios`: If you receive historical data (e.g., monthly CSV dumps), you can crawl and immediately make them queryable before cleaning.
  - `Glue Catalog integration`: Even raw data tables are needed if you’re building full lineage tracking or a central catalog.
  - `Learning/debugging`: Crawling raw formats helps beginners understand partitioning, formats, compression effects, etc.

## Analytics

- Athena Queries:
  - daily sales
  - popular products
  - sales by country
  - top customers
- Quicksight Charts:
  <p align="center">
    <img src="/analytics/quicksight_imgs/daily_sales.png" alt="daily_sales" width="700" height="430">
  </p> 
  <p align="center">
    <img src="/analytics//quicksight_imgs/popular_products.png" alt="popular_products" width="700" height="430">
  </p> 
  <p align="center">
    <img src="/analytics//quicksight_imgs/sales_by_country.png" alt="sales_by_country" width="700" height="430">
  </p> 
  <p align="center">
    <img src="/analytics//quicksight_imgs/top_customers.png" alt="top_customers" width="700" height="430">
  </p>

## ML

This layer focuses on preparing data, training a model, and performing inference — ideally leveraging real-world workflows.

- Goals:

  - High-value invoices: To build an ML pipeline that predicts whether an invoice is high-value, as a first real ML application on the ecommerce dataset.

    - Folder structure:

      ```bash
      ml/
      ├── classification/
      │   ├── train.py          # trains the model to classify invoices
      │   ├── predict_high_value.py  # uses the model to make predictions
      │   └── requirements.in   # ML-specific dependencies
      ├── data_prep/
      │   ├── clean_to_ml.py    # prepares a labeled dataset from the clean zone
      ```

    - Steps:

      1. Data Preparation (`in data_prep/`)

      - features:
        - product_count: number of projects in the invoice
        - total_amount: quantity \* unit price
      - label each invoice as `high-value` if `total_amount>=1000`
      - save to csv file

      2. Model Training (`in classification/train.py`)

      - read the csv file
      - train a `RandomForestClassifier` on the features
      - save the model in a `.joblib` file
      - Evaluated with precision/recall/F1: ✅ All were 1.0 (overfitting expected on small dataset)

      3. Inference (`in classification/predict_high_value.py`)

      - load the model and test the model with test samples matching the training schema
      - Got expected predictions (1 for high-value, 0 for not)

    - what I learned
      - ML = Data + Model + Purpose:
        - You need a clear goal (e.g., invoice classification) and clean features.
      - Data schema matters:
        - The model expects inputs to match the training columns exactly — in name, number, and order.
      - Training vs Inference:
        - Training builds a model using known outcomes.
        - Inference applies that model to new, unknown cases.
      - Tools used:
        - pandas: data handling
        - sklearn: modeling
        - joblib: model serialization
        - awswrangler: S3 + Glue-friendly data loading

  - customer segmentation: group customers into meaningful segments to understand their behavior and support personalization, marketing, and retention strategies.

    <p align="center">
    <img src="/ml/segmentation/imgs/customer_segments(PCA Projection).png" alt="customer_segments" width="700" height="430">
    </p>

    - Folder Structure

      ```bash
      ml/
      ├── segmentation/
      │   └── imgs/
      │   │   └── customer_segments(PCA Projection).png  # K-Means clusters
      │   ├── cluster_summary.py  # print the summary of clusters and interpret them
      │   ├── cluster.py          # use KMeans model to segment clients into clusters
      │   ├── data_prep.py          # create a customer feature csv from clean_batch glue table
      │   ├── label_cluster.py          # attach labels to each cluster
      │   ├── main_segment_query.sql    # the Athena query joining the "segment" table and "clean_batch" table
      │   ├── requirements.in          # ML-specific dependencies
      │   ├── upload_labeled_segments.py          # upload the labeled_customer_segments.csv to s3
      │   ├── visualize.py          # visualize the clusters of clients
      ```

    - Steps:

      1. Data Preparation (data_prep.py)

      - Aggregated transactional data into customer-level features:
        - total_spent, order_count, avg_basket_size, recency_days
      - Cleaned and exported as customer_metrics.csv

      2. Clustering with K-Means (cluster.py)

      - Standardized features using StandardScaler
      - Chose number of clusters (k=4) based on domain intuition
      - Fitted KMeans and labeled each customer with a cluster

      3. Cluster Interpretation

      - Analyzed cluster centroids to infer behavior:
        - e.g., "Elite VIP Customers", "Churned", "New Low Spenders", etc.

      4. Customer Labeling (label.py)

      - Attached readable segment names to each customer
      - Saved as labeled_customer_segment.csv

      5. Glue + Athena Integration

      - Uploaded segment data to S3
      - Added crawler target and reran crawler
      - Joined labeled segments with transactional data in Athena for analysis

    - what I learned
      - The concept of unsupervised learning —- no labels, only patterns
      - How clustering works in practice (distance, centroids, feature scaling)
      - How to interpret clusters using real data
      - How to join aggregated features back with the original dataset - How segmentation insights can support targeted business actions
      <!-- - `rag/app/`: Frontend + API layer for demoing RAG interactions -->

  - Product Recommendation with ALS: to build a product recommendation system that suggests relevant products to users based on their past purchasing behavior.

    - tools & libraries

      - pandas: for data wrangling
      - scipy.sparse: to build sparse matrices
      - implicit: ALS-based collaborative filtering model
      - joblib: model persistence

    - Dataset: we prepare the _dataset_ out of _temp.csv_, which was generated by the file: `ingestion/batch_ingestion/batch_loader.py` instead of reading the data via AWS Glue Table: `clean_table` to save costs. The input for recommendation is a user-item interaction matrix, built by aggregating quantities of each product purchased by each customer.

    - Steps:

      1. Data Preparation (data_prep.py)

      - Source: temp.csv (raw data)
      - Transformations:
        - Drop rows with null CustomerID, InvoiceDate, or Description
        - Normalize column types: convert CustomerID and StockCode to strings
        - Aggregate Quantity by (CustomerID, StockCode) to get TotalQuantity
        - Output saved to user_item_interactions.csv

      2. Model Training (train.py)

      - Built sparse matrix using coo_matrix
      - Categorical encoding of users/items for internal mapping
      - Trained ALS model with
      - Persisted model and mappings with joblib
      - Defensive checks:
        - Filtered out rows with non-numeric or missing CustomerID, StockCode
        - Removed rows where TotalQuantity <= 0

      3. Model Prediction (predict.py)

      - Load model and mappings
      - Convert input CustomerID to internal ALS ID
      - Generate Top-5 recommended item IDs
      - Map internal item IDs back to original StockCode
      - Handle edge cases:
        - Unmapped customer or item IDs
        - Missing products in the final mapping due to bad training records

    - insights:

      - Sparse Matrix: Enabled efficient computation for tens of thousands of user-item interactions.
      - Latent Factors: Each user and item was projected into a 50-dimensional space capturing behavioral signals.
      - Data Cleanliness: Crucial to success. We observed that bad or missing StockCodes can silently reduce mapping consistency.

    - What I learned:
      - How to build a recommendation system using implicit matrix factorization (ALS)
      - The importance of data preprocessing in collaborative filtering
      - How mappings between raw IDs and internal model IDs are essential
      - The role of sparse matrices and how implicit leverages them
      - The distinction between training-time mappings and inference-time logic

## Notebooks

This section of the project provided hands-on experience with various exploratory and diagnostic tasks using Python, pandas, and visualization libraries. The main goal was to understand, clean, profile, and visualize data — critical steps that bridge raw data and applied ML/AI work.

- Exploratory Data Analysis (EDA): Get a basic understanding of the structure, distributions, and relationships in the data.
  - Dataset Used: user_item_interactions.csv (from the recommendation layer)
  - Libraries: pandas, seaborn, matplotlib
  - Tasks Completed:
    - Viewed data structure and sample rows
    - Checked data types and missing values
    - Generated distribution plots and bar charts for quantity, customer activity, and item frequency
  - Learnings:
    - EDA is the first and essential step before modeling.
    - Seaborn makes it easy to create meaningful visualizations from tabular data.
- Data Profiling: Auto-generate a comprehensive report of the dataset to inspect data types, correlations, missing values, and distributions.
  - Tool Used: ydata-profiling (formerly pandas-profiling)
  - Output: profiling_report.html
  - Learnings:
    - Profiling tools can save time by offering a full overview at once.
    - Useful for spotting anomalies or insights (e.g., skewed columns, strong correlations).
    - Integration with notebooks or scripts is straightforward.
    - Minor package compatibility issues were resolved by isolating into a temporary virtual environment.
- Data Cleaning: Apply typical data cleaning techniques to improve data quality and consistency.
  - Tasks Completed:
    - Removed missing and duplicate entries
    - Standardized column names and date formats
    - Added derived column TotalPrice = Quantity × UnitPrice
    - Saved the cleaned dataset for downstream tasks
  - Learnings:
    - Cleaning logic is often dataset-specific and iterative.
    - Important to check for weird values and structural inconsistencies (e.g., negative prices).
- Feature Engineering: Enhance the dataset with new, meaningful variables derived from raw columns to support analysis and modeling.
  - Tasks Completed:
    - Created TotalPrice = Quantity × UnitPrice
    - Extracted InvoiceDate into datetime format (enabling time-based analysis)
    - Aggregated data by customer and item to derive spend and frequency patterns (used later in segmentation and recommendation tasks)
  - Learnings:
    - Feature engineering transforms data from "raw signals" to "actionable insights".
    - It bridges raw values and ML-ready inputs (e.g., customer behavior metrics).
    - Small transformations (like TotalPrice) can drastically increase analytical power.
- Visualization: Build more advanced visualizations to explore customer behavior and purchase patterns.

  - Tasks Completed:
    - Line plot of total sales over time
    - Bar plots for top customers and top products
    - Libraries: Combined matplotlib (for figure handling) and seaborn (for styling and plotting)
  - Generated Charts:
    <p align="center">
    <img src="/notebooks/notebooks_visual/fig_total_spent_vs_order_count.png" alt="spent_vs_count" width="700" height="500">
    </p>
    <p align="center">
    <img src="/notebooks/notebooks_visual/fig_total_spent_vs_avg_basket_size.png" alt="spent_vs_basket" width="700" height="500">
    </p>
    <p align="center">
    <img src="/notebooks/notebooks_visual/fig_recency_vs_total_spent.png" alt="spent_vs_recency" width="700" height="500">
    </p>
    <p align="center">
    <img src="/notebooks/notebooks_visual/fig_correlation_matrix.png" alt="customer correlation matrix" width="700" height="500">
    </p>
  - Learnings:
    - Visualization can instantly reveal trends and anomalies.
    - matplotlib provides base plotting while seaborn adds statistical and stylistic power
    - It’s common to mix them — e.g., setting up a fig with matplotlib and plotting with seaborn

- Final Notes:

  - A temporary virtual environment was used for profiling to avoid CDK conflicts (due to typeguard dependency).
  - The generated HTML profiling report was excluded via .gitignore to avoid bloating the repo.
  - This section helped reinforce how raw datasets must be explored and understood before modeling.

- RAG

  - Summary: The RAG layer was designed to enable intelligent, factual product Q&A using LLMs grounded in our own product knowledge data. This approach reduces hallucinations and improves answer reliability by allowing the model to "look up" answers in a vector database populated with actual product information.

  - Goal: build a lightweight local RAG pipeline that allows users to ask questions about ecommerce products based on the `temp.csv` (raw batch) file used in the `ML recommendation goal`.

  - Pipeline Summary
    - Step 1: Prepare Product Documents
      - File: prepare_product_knowledge.py
      - Reads `temp.csv`, performs light cleanup and feature engineering.
      - Aggregates data per product (StockCode).
      - Outputs a Markdown document product_knowledge.md.
      - This document contains per-product summaries with total sales, revenue, countries, and time range.
    - Step 2: Generate Embeddings + Save Index
      - File: prepare_embeddings.py
      - Loads the product_knowledge.md file.
      - Splits the content into ~500-character chunks using LangChain's RecursiveCharacterTextSplitter.
      - Embeds the chunks using sentence-transformers/all-MiniLM-L6-v2.
      - Stores the vector index and metadata to faiss_index/index.faiss and index.pkl.
    - Step 3: Run Queries via LLM
      - File: rag_query.py
      - Accepts natural language user input.
      - Embeds the query and performs FAISS similarity search (k=5).
      - Feeds the top matching chunks to OpenAI GPT-3.5 via the chat.completions.create() API.
      - Prints the model's answer to the terminal.
  - Setup Notes
    - A new virtual environment was created under rag/app/.
    - Requirements were split:
      - requirements.in for runtime: faiss-cpu, sentence-transformers, openai, python-dotenv
      - requirements-dev.in (optional): langchain, joblib
    - .env stores the OpenAI API key.
  - Insights & Key Learnings
    - Why Use RAG
      - Traditional LLMs can hallucinate or give vague answers.
      - RAG grounds responses in real, verifiable data.
    - Importance of Document Preparation
      - We can’t search what doesn’t exist — LLM quality depends heavily on the quality and coverage of indexed chunks.
      - This is equivalent to "designing access patterns" in databases.
    - Role of Vector DB (FAISS)
      - FAISS doesn’t query fields or perform logic — it just returns top-K similar chunks.
      - The LLM extracts structured understanding from those chunks.
    - LLM Role
      - It interprets the user’s question and answers based on the context provided.
      - It doesn’t know your data unless you give it meaningful input.
  - Closing Thoughts
    - This RAG layer marks the final component of the project and ties together everything we’ve built:
      - Clean data 🧼 (ETL)
      - Insightful analytics 📊
      - Predictive modeling 🤖
      - Factual question-answering with generative AI 💬

## Virtual Environment and Package Management

- Current Design: For package management, this project is using one single virtual environment, but with multiple requirements.txt files. As such, we could relatively easily manage packages for each of our sub projects as well as the main project. In order to do it, we use a tool called `pip-compile` with multiple `requirements.in` files.

  > note: for some sub project, in order to avoid package conflicts, we created separated virtual environments. The related projects are: `notebooks`,`rag/app`

- Future Challenge & Solution: Right now, we are using one single virtual environment for our project, which consists of mutliple sub projects. However, as our project grows, we may encounter issues such as `Potential version conflicts`. And when that moment comes, we could switch to a `Layered` approach with `pip install -e`. In other words, we can make the main project installable as a package, then install it in subprojects using editable mode. By doing this, our code base will be more `Clean, modular, real-Pythonic`.
  > Note: since we are already using `pip-compile`, it will make later migration more smooth because every sub project has its own `requirements.in`. So with this setting, it is safe, compatible, future-proof.
